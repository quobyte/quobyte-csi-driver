#!/bin/bash

MOUNT_WITH_ACCESS_KEYS=${MOUNT_WITH_ACCESS_KEYS:-n}
PATH="$(pwd):$PATH"
CODE_BASE_DIR="$(pwd)"
POD_KILLER_DIR="$(pwd)/../quobyte-csi-pod-killer"
TEST_CASE_DIR="${TEST_CASE_DIR:-}"
KIND_CLUSTER_NAME="quobyte-csi-testing"
TEST_CLUSTER_DIR='kind-cluster/kind-csi-testing'
CSI_PROVISIONER_NAME=${CSI_PROVISIONER_NAME:-'csi.quobyte.com'}
# do not tag version as latest - that would trigger pull always from registry
CSI_DRIVER_VERSION="$(git rev-parse --short HEAD)"
# dummy.quobyte.com is not a docker registry. We buid docker image locally
# and make it available to kind nodes via 'kind load docker-image'
CSI_DRIVER_IMAGE="dummy.quobyte.com/quobyte-csi-driver:${CSI_DRIVER_VERSION}"
KUBECONFIG='/tmp/quobyte-k8s-config'

die() { echo "$*" 1>&2 ; exit 1; }

if [ ! -z "$(git status --porcelain)" ]; then
  echo 'Requires clean directory (no stage/unstaged/untracked files) in repo'
  exit 1
fi

if [ ! -d "${POD_KILLER_DIR}" ]; then
  die "Pod killer code base does not exists in the path ${POD_KILLER_DIR}"
fi

echo "*** Building pod killer ***"
cd ${POD_KILLER_DIR}
POD_KILLER_VERSION="$(git rev-parse --short HEAD)"
POD_KILLER_CONTAINER_URL_BASE="dummy.quobyte.com/pod-killer"
POD_KILLER_IMAGE="${POD_KILLER_CONTAINER_URL_BASE}:${POD_KILLER_VERSION}"
${POD_KILLER_DIR}/build
docker build -t ${POD_KILLER_IMAGE} -f Dockerfile .
pod_killer_container_build_status=$?

if [[ "${pod_killer_container_build_status}" -ne 0 ]]; then
  die "Failed building pod killer container"
else
  echo "Successfully build pod killer image $POD_KILLER_IMAGE"
fi

cd $CODE_BASE_DIR
if [[ -f "$KUBECONFIG" ]]; then
  rm "$KUBECONFIG"
fi

# set kubeconfig so that kubectl in this context can with the kind cluster configuration
export KUBECONFIG="${KUBECONFIG}"

tee -a "$KUBECONFIG" <<END
apiVersion: v1
kind: Config
preferences: {}
END

rm -rf "${TEST_CLUSTER_DIR}"

mkdir -p "${TEST_CLUSTER_DIR}"

echo "Running script from $(pwd)"

echo "Creating kind k8s cluster Dockerfile * * * * * * * * *  * * * * * * * * * * * * * * * * *"
echo ""
echo ""
tee -a "${TEST_CLUSTER_DIR}"/Dockerfile <<END
FROM kindest/node:v1.30.6@sha256:b6d08db72079ba5ae1f4a88a09025c0a904af3b52387643c285442afb05ab994
RUN apt-get -y update 
RUN apt-get -y install wget && apt-get install -y git && apt install -y nano dnsutils
END

echo "Building new image using the above Dockerfile. On the local machine: * * *"
# Change `v0` appropriately to whatever version is needed.
docker build -t 'quobyte/kind-node-testing:v0' -f "${TEST_CLUSTER_DIR}"/Dockerfile .

docker image ls | grep 'quobyte/kind-node-testing:v0'

echo "Use the newly created image in the kind cluster configuration: * * * * * * * * *"
tee -a "${TEST_CLUSTER_DIR}"/kind-config-testing.yaml <<END
kind: Cluster
name: ${KIND_CLUSTER_NAME}
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
END

echo  "Creating cluster with kind: * * * * * * * * * * * * * * * * * * * * * * *"
kind create cluster --config="${TEST_CLUSTER_DIR}"/kind-config-testing.yaml --kubeconfig ${KUBECONFIG}

verify_all_nodes_ready() {
  local -n result=$1
  has_not_ready_nodes="0"
  let current_time=0
  while [[ $has_not_ready_nodes -eq 0 && $current_time -le 300 ]]; do
    kubectl get nodes  | awk 'NR>1{print $2}' | grep -q 'NotReady'
    has_not_ready_nodes="$?"
    if [[ "$has_not_ready_nodes" -eq 0 ]]; then
      echo -e "\e[33mK8S node(s) is not ready yet, will check again in 10s\e[0m"
      sleep 10s
      current_time=$(($current_time + 10))
    fi
  done
  result=$has_not_ready_nodes
}

verify_all_nodes_ready has_not_ready_nodes
if [[ "$has_not_ready_nodes" -eq 0 ]]; then
  echo ""
  kubectl get nodes | grep -q "NotReady" | awk 'NR>1{print $1}' \
  | xargs -I {} sh -c "echo "Restarting the node container {}"; sudo docker restart {}"
fi
verify_all_nodes_ready has_not_ready_nodes
if [[ "$has_not_ready_nodes" -eq 0 ]]; then
  echo "K8S nodes are not ready after 10 minutes (Restarting also not fixed the nodes ready issue). \
       Cannot continue testing cluster setup. Please fix the node not ready issue."
fi
# Wait until nodes are ready for DNS entries to be added - adding DNS entries before nodes are ready
# makes nodes hang in NotReady
dns_ips="$(kubectl get services -n kube-system | grep 'kube-dns' | awk '{print $3}')"
if [[ -z "$dns_ips" ]]; then
  echo "Error: could not retrive kube-dns IP required"
  exit 1
fi
if ! [[ "$dns_ips" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
  echo "Error: Invalid kube-dns IP $dns_ips"
  exit 1
fi

# Add host dns entries, so that we can resolve all the names (dependent container urls, Quobyte
# api/registry etc)
for host_dns_ip in $(grep nameserver /etc/resolv.conf | awk '{print $2}'); do
   dns_ips="$dns_ips $host_dns_ip"
done

# Requires for Quobyte pod killer components - cache is run as a service and mount monitor
# uses service url to communicate with the cache. Without kube-dns mapped, mount monitor cannot
# resolve the service url
echo "Mapping k8s kube-dns resolver into k8s nodes * * * * * * * * *"
for node in $(kubectl get nodes --no-headers | grep 'worker' | awk '{print $1}'); do
  echo "Mapping k8s kube-dns resolver into $node"
  # Nullify current configuration -- ATM, kind configures dns is configured with the default
  # gateway of the virtual network
  sudo docker exec -it $node sh -c "cat /dev/null > /etc/resolv.conf"
  # Next couple of lines are not intended on purpose - otherwise command does not work as
  # -c interprets text verbatim
  sudo docker exec -it $node sh -c "cat > /etc/resolv.conf <<END
search default svc.cluster.local cluster.local
END"
for dns_ip in $dns_ips; do
sudo docker exec -it $node sh -c "cat >> /etc/resolv.conf <<END
nameserver $dns_ip
END"
done
 sudo docker exec -it $node sh -c "cat >> /etc/resolv.conf <<END
options timeout:1 ndots:0
END"
  echo "Configured worker node $node with DNS entries as following:"
  sudo docker exec -it quobyte-csi-testing-worker sh -c "cat /etc/resolv.conf"
  echo ""
done

echo  "Labeling Kubernetes nodes for Quobyte client installation * * * * * * * * * * * * * * *"
for node in $(kubectl get nodes --no-headers | awk '{print $1}'); do
  kubectl label nodes $node quobyte_client="true"
done

echo "Building local csi driver image"
${CODE_BASE_DIR}/src/build.sh
docker build -t ${CSI_DRIVER_IMAGE} -f ${CODE_BASE_DIR}/src/Dockerfile ${CODE_BASE_DIR}/src
if [[ "$?" -ne 0 ]]; then
  echo "Failed to create local image (compile or image build failed!)"
  exit 1
fi
kind load docker-image ${CSI_DRIVER_IMAGE} --name ${KIND_CLUSTER_NAME}
kind load docker-image ${POD_KILLER_IMAGE} --name ${KIND_CLUSTER_NAME}

echo "creating and setting default namespace to quobyte * * * * * * * * * * * *" 
#to retrieve instances from all namespaces use '-A' argument
kubectl create ns quobyte
kubectl config get-contexts
kubectl get namespaces --context kind-quobyte-csi-experiment-testing
kubectl config use-context kind-quobyte-csi-experiment-testing
kubectl config set-context --current --namespace=quobyte

echo "To access the test cluster you should export KUBECONFIG=$KUBECONFIG env varialbe"

if [[ -z "$TEST_CASE_DIR" ]]; then
  echo "No TEST_CASE_DIR is provided with test case configuration"
  echo "Set values.yaml with your Quobyte API and quobyte.dev.csiImage: ${CSI_DRIVER_IMAGE}"
  echo "Install a Quobyte client (example/client.yaml with adjusted namespace and Quobyte registr)"
  echo "  then install Quobyte CSI driver manually using helm (helm install quobyte-csi-driver ../csi-driver-templates)"
  exit 1
fi

verify_namespace_has_no_crashing_pods() {
  namespace=''
  if [[ -z "$1" ]]; then
    echo 'No namespace provided for pod health check. Defaulting to --all-namepaces namespace'
    namespace="--all-namespaces"
  else
    namespace="$1"
  fi
  has_crashing_pods=0
  count=1
  while [[ $has_crashing_pods -eq 0 && count -le 10 ]]; do
    kubectl get po -n $namespace | awk 'NR>1{print $3}' | grep -iv 'running'
    # inverted match - result is 0 if at least one pod is !running, 1 - if all are running
    has_crashing_pods="$?"
    let count++
    if [[ $has_crashing_pods -eq 0 ]]; then
      sleep 1m
      echo "Waiting for all pods in namespace $namespace to be in Running state. Time elapsed $count minutes"
    fi
  done
  if [[ $has_crashing_pods -eq 0 ]]; then
    echo "Some of the pods in namespace $namespace are not in Running state after $count minutes."
    echo "Get pods in namespace $namespace, describe crashing pod(s) to find the crash reason and retyin fixing it."
    echo "This indicates a problem with the driver setup, therefore exiting with error code 1"
    exit 1
  fi
}

function check_snapshots_resources_exists() {
  if [[ ! -f "$CODE_BASE_DIR/csi-driver-templates/k8s-snapshot-crd.yaml" ]]; then
    echo "ERROR: k8s snapshot CRD definitions not found in ${CODE_BASE_DIR}/csi-driver-templates/k8s-snapshot-crd.yaml"
    exit 1
  fi
  if [[ ! -f "$CODE_BASE_DIR/csi-driver-templates/k8s-snapshot-controller.yaml" ]]; then
    echo "ERROR: k8s snapshot controller definition not found in ${CODE_BASE_DIR}/csi-driver-templates/k8s-snapshot-controller.yaml"
    exit 1
  fi
}


grep -E "enableSnapshots:.*true" "${TEST_CASE_DIR}/values.yaml" &> /dev/null
enabled_snapshots=$?
if [[ "${enabled_snapshots}" -eq 0 ]]; then
  check_snapshots_resources_exists
  kubectl create -f "$CODE_BASE_DIR/csi-driver-templates/k8s-snapshot-crd.yaml"
  sleep 10s
  kubectl create -f "$CODE_BASE_DIR/csi-driver-templates/k8s-snapshot-controller.yaml"
fi

echo "Deploying k8s resources (k8s_<...>.yaml) files in test dir $TEST_CASE_DIR"
for file in ${TEST_CASE_DIR}/k8s_*.yaml ; do kubectl apply -f "$file"; done

sleep 30s

if [[ ! -f "${TEST_CASE_DIR}/values.yaml" ]]; then
  die "${TEST_CASE_DIR}/values.yaml not found. Cannot setup Quobyte CSI Driver."
fi

echo ""
echo "Deploying CSI Driver with $TEST_CASE_DIR/values.yaml with overrides \
      quobyte.dev.csiProvisionerVersion=$CSI_DRIVER_VERSION \
      quobyte.dev.csiImage=$CSI_DRIVER_IMAGE \
      quobyte.dev.podKillerImage=$POD_KILLER_IMAGE \
      quobyte.csiProvisionerName=$CSI_PROVISIONER_NAME"
# install CSI driver with the custom image
helm install quobyte-csi-driver ./csi-driver-templates -f ${TEST_CASE_DIR}/values.yaml \
 --set quobyte.dev.csiProvisionerVersion="$CSI_DRIVER_VERSION" \
 --set quobyte.dev.csiImage="$CSI_DRIVER_IMAGE" \
 --set quobyte.dev.podKillerImage="$POD_KILLER_IMAGE" \
 --set quobyte.csiProvisionerName="$CSI_PROVISIONER_NAME"

sleep 1m
verify_namespace_has_no_crashing_pods

echo "List all pods"
kubectl get po --all-namespaces

ENABLE_SNAPSHOTS='false'
if [[ "$enabled_snapshots" -eq 0 ]]; then
  ENABLE_SNAPSHOTS='true'
fi

echo ""
if [[ ! -f "$TEST_CASE_DIR/k8s_storage_class.yaml" ]]; then
  echo "No k8s_storage_class.yaml exists in the given test path $TEST_CASE_DIR. So not running e2e tests"
  exit 0  # success as driver is setup with given values.yaml 
fi
echo "Running e2e tests the storage class in $TEST_CASE_DIR/k8s_storage_class.yaml"
STORAGE_CLASS="$TEST_CASE_DIR/k8s_storage_class.yaml" KUBECONFIG="$KUBECONFIG" \
 SNAPSHOT_CLASS="$TEST_CASE_DIR/k8s_volume-snapshot-class.yaml" \
 CSI_PROVISIONER_NAME="$CSI_PROVISIONER_NAME" ENABLE_SNAPSHOTS="$ENABLE_SNAPSHOTS" kind-cluster/e2e \
 || die 'Failed running e2e tests'
