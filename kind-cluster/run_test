#!/bin/bash

MOUNT_WITH_ACCESS_KEYS=${MOUNT_WITH_ACCESS_KEYS:-n}
PATH="$(pwd):$PATH"
CODE_BASE_DIR="$(pwd)"
TEST_CASE_DIR="${TEST_CASE_DIR:-}"
TEST_CLUSTER_DIR='kind-cluster/kind-csi-experiment-testing-corp'
KIND_CLUSTER_NAME="quobyte-csi-experiment-testing"
CSI_PROVISIONER_NAME=${CSI_PROVISIONER_NAME:-'csi.quobyte.com'}
# do not tag version as latest - that would trigger pull always from registry
CSI_DRIVER_VERSION="$(git rev-parse --short HEAD)"
# dummy.quobyte.com is not a docker registry. We buid docker image locally
# and make it available to kind nodes via 'kind load docker-image'
CSI_DRIVER_IMAGE="dummy.quobyte.com/quobyte-csi-driver:${CSI_DRIVER_VERSION}"
KUBECONFIG='/tmp/quobyte-k8s-config'

die() { echo "$*" 1>&2 ; exit 1; }

if [ ! -z "$(git status --porcelain)" ]; then
  echo 'Requires clean directory (no stage/unstaged/untracked files) in repo'
  exit 1
fi

if [[ -f "$KUBECONFIG" ]]; then
  rm "$KUBECONFIG"
fi

# set kubeconfig so that kubectl in this context can with the kind cluster configuration
export KUBECONFIG="${KUBECONFIG}"

tee -a "$KUBECONFIG" <<END
apiVersion: v1
kind: Config
preferences: {}
END

rm -rf "${TEST_CLUSTER_DIR}"

mkdir -p "${TEST_CLUSTER_DIR}"

echo "Running script from $(pwd)"

echo "Creating a Dockerfile * * * * * * * * *  * * * * * * * * * * * * * * * * *"
echo ""
echo ""
tee -a "${TEST_CLUSTER_DIR}"/Dockerfile <<END
FROM kindest/node:v1.26.0
RUN apt-get -y update 
RUN apt-get -y install wget && apt-get install -y git && apt install -y nano
END

echo "Building new image using the above Dockerfile. On the local machine: * * *"
# Change `v0` appropriately to whatever version is needed.
docker build -t 'quobyte/kind-node-testing:v0' -f "${TEST_CLUSTER_DIR}"/Dockerfile .

docker image ls | grep 'quobyte/kind-node-testing:v0'

echo "Use the newly created image in the kind cluster configuration: * * * * * * * * *"
tee -a "${TEST_CLUSTER_DIR}"/kind-config-testing.yaml <<END
kind: Cluster
name: ${KIND_CLUSTER_NAME}
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
- role: worker
  image: quobyte/kind-node-testing:v0
END

echo  "Creating cluster with kind: * * * * * * * * * * * * * * * * * * * * * * *"
kind create cluster --config="${TEST_CLUSTER_DIR}"/kind-config-testing.yaml --kubeconfig ${KUBECONFIG}

echo  "Labeling Kubernetes nodes * * * * * * * * * * * * * * * * * * * * * * * *"
for node in $(kubectl get nodes --no-headers | awk '{print $1}'); do
  kubectl label nodes $node quobyte_client="true"
done

echo "Building local csi driver image"
${CODE_BASE_DIR}/src/build.sh
docker build -t ${CSI_DRIVER_IMAGE} -f ${CODE_BASE_DIR}/src/Dockerfile ${CODE_BASE_DIR}/src
if [[ "$?" -ne 0 ]]; then
  echo "Failed to create local image (compile or image build failed!)"
  exit 1
fi
kind load docker-image ${CSI_DRIVER_IMAGE} --name ${KIND_CLUSTER_NAME}

echo "creating and setting default namespace to quobyte * * * * * * * * * * * *" 
#to retrieve instances from all namespaces use '-A' argument
kubectl create ns quobyte
kubectl config get-contexts
kubectl get namespaces --context kind-quobyte-csi-experiment-testing
kubectl config use-context kind-quobyte-csi-experiment-testing
kubectl config set-context --current --namespace=quobyte

echo "To access the test cluster you should export KUBECONFIG=$KUBECONFIG env varialbe"

if [[ -z "$TEST_CASE_DIR" ]]; then
  echo "No TEST_CASE_DIR is provided with test case configuration"
  echo "Set values.yaml with your Quobyte API and quobyte.dev.csiImage: ${CSI_DRIVER_IMAGE}"
  echo "Install a Quobyte client (example/client.yaml with adjusted namespace and Quobyte registr)"
  echo "  then install Quobyte CSI driver manually using helm (helm install quobyte-csi-driver ../csi-driver-templates)"
  exit 1
fi

verify_namespace_has_no_crashing_pods() {
  namespace=''
  if [[ -z "$1" ]]; then
    echo 'No namespace provided for pod health check. Defaulting to --all-namepaces namespace'
    namespace="--all-namespaces"
  else
    namespace="$1"
  fi
  has_crashing_pods=0
  count=1
  while [[ $has_crashing_pods -eq 0 && count -le 10 ]]; do
    kubectl get po -n $namespace | awk 'NR>1{print $3}' | grep -iv 'running'
    has_crashing_pods="$?"
    let count++
    if [[ has_crashing_pods -eq 0 ]]; then
      sleep 1m
      echo "Waiting for all pods in namespace $namespace to be in Running state. Time elapsed $count minutes"
    fi
  done
  if [[ has_crashing_pods -eq 0 ]]; then
    echo "Some of the pods in namespace $namespace are not in Running state after $count minutes."
    echo "Get pods in namespace $namespace, describe crashing pod(s) to find the crash reason and retyin fixing it."
    echo "This indicates a problem with the driver setup, therefore exiting with error code 1"
    exit 1
  fi
}

echo "Deploying k8s resources (k8s_<...>.yaml) files in test dir $TEST_CASE_DIR"
for file in ${TEST_CASE_DIR}/k8s_*.yaml ; do kubectl apply -f "$file"; done

sleep 30s

echo ""
echo "Deploying CSI Driver with $TEST_CASE_DIR/values.yaml and quobyte.dev.csiImage set to $CSI_DRIVER_IMAGE"
# install CSI driver with the custom image
helm install quobyte-csi-driver ./csi-driver-templates -f ${TEST_CASE_DIR}/values.yaml \
 --set quobyte.dev.csiProvisionerVersion="$CSI_DRIVER_VERSION" \
 --set quobyte.dev.csiImage="$CSI_DRIVER_IMAGE" \
 --set quobyte.csiProvisionerName="$CSI_PROVISIONER_NAME"

sleep 1m
verify_namespace_has_no_crashing_pods

echo "List all pods"
kubectl get po --all-namespaces

echo ""
echo "Running e2e tests the storage class in $TEST_CASE_DIR/k8s_storage_class.yaml"
STORAGE_CLASS="$TEST_CASE_DIR/k8s_storage_class.yaml" KUBECONFIG="$KUBECONFIG" \
 CSI_PROVISIONER_NAME="$CSI_PROVISIONER_NAME" kind-cluster/e2e \
 || die 'Failing e2e tests with api and mount secrets'
